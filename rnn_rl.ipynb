{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kTFLzE6iwD8",
        "outputId": "43d10603-be99-4b40-eb11-84c121b1447d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainX: (101, 12)\n",
            "trainY: (101,)\n",
            "testX: (17, 12)\n",
            "testY: (17,)\n"
          ]
        }
      ],
      "source": [
        "# PID 5122089\n",
        "# Name Adam Farrow\n",
        "# Collaboration:\n",
        "# N/A\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Step 1. Use pandas to read training and testing from txt file.\n",
        "train = read_csv(\"train.txt\")\n",
        "test = read_csv(\"test.txt\")\n",
        "\n",
        "train_data = train[\"Passengers\"].values.astype(np.float32)\n",
        "test_data = test[\"Passengers\"].values.astype(np.float32)\n",
        "\n",
        "\n",
        "# Step 2. Normalize training and test data into [0, 1]. \n",
        "scaler = MinMaxScaler()\n",
        "train_data_normalized = scaler.fit_transform(train_data.reshape(-1, 1)).flatten()\n",
        "test_data_normalized = scaler.transform(test_data.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Step 3. Create a training and test datasets. \n",
        "x, y = [], []\n",
        "for i in range(len(train_data_normalized) - 12):\n",
        "    x.append(train_data_normalized[i:i+12])\n",
        "    y.append(train_data_normalized[i+12])\n",
        "\n",
        "trainX = np.array(x)\n",
        "trainY = np.array(y)\n",
        "\n",
        "x, y = [], []\n",
        "for i in range(len(test_data_normalized) - 12):\n",
        "    x.append(test_data_normalized[i:i+12])\n",
        "    y.append(test_data_normalized[i+12])\n",
        "\n",
        "testX = np.array(x)\n",
        "testY = np.array(y)\n",
        "\n",
        "# Step 4. Print out the shape of data.\n",
        "print(\"trainX:\", trainX.shape)\n",
        "print(\"trainY:\", trainY.shape)\n",
        "print(\"testX:\", testX.shape)\n",
        "print(\"testY:\", testY.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3I5ig_DlFhw",
        "outputId": "58ae692f-0eba-4151-c2d9-afc86dbbd710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100 Loss: 0.0453\n",
            "Epoch 200 Loss: 0.0039\n",
            "Epoch 300 Loss: 0.0069\n",
            "Epoch 400 Loss: 0.0092\n",
            "Epoch 500 Loss: 0.0026\n",
            "Epoch 600 Loss: 0.0039\n",
            "Epoch 700 Loss: 0.0043\n",
            "Epoch 800 Loss: 0.0068\n",
            "Epoch 900 Loss: 0.0047\n",
            "Epoch 1000 Loss: 0.0063\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "# Complete the model architecture \n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "      super(RNN, self).__init__()\n",
        "      self.hidden_size = hidden_size\n",
        "      self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "      self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      out, _ = self.rnn(x)\n",
        "      out = self.fc(out[:, -1, :])\n",
        "      return out\n",
        "\n",
        "# Create an instance of model, optimizer and criterion. \n",
        "model = RNN(1, 4, 1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "# Train the RNN Model for 1000 epoch and print out the training loss for every 100 epochs. \n",
        "for epoch in range(1000):\n",
        "    #we will use np.random to help generate random batches of size 10\n",
        "    indices = np.random.randint(0, len(trainX), 10)\n",
        "    inputs = torch.tensor(trainX[indices], dtype=torch.float32)\n",
        "    targets = torch.tensor(trainY[indices], dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs.unsqueeze(-1))\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(outputs.squeeze(), targets)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print training every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch {epoch+1} Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQkkgTCWmg-l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# Step 1. Load IMDB dataset from keras. \n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
        "\n",
        "# Step 2. Preprocess the sequences with padding \n",
        "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlzYDdE2ssF6",
        "outputId": "9eb2ab5a-2fd7-4c80-e41e-b16347ca51ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best validation accuracy: 0.8167\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define LSTM model architecture\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, max_features, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # define the embedded , LSTM, and Linear layer\n",
        "        self.embeddings = nn.Embedding(max_features, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.linear = nn.Linear(hidden_dim, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # defining forward pass\n",
        "\n",
        "        # embedding layer\n",
        "        x = self.embeddings(x)\n",
        "        # then we need to permute the dimensions for the LSTM input\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # we then initialize the hidden state and the cell state tensors\n",
        "        h_0 = torch.zeros(1, x.size(1), 8)\n",
        "        c_0 = torch.zeros(1, x.size(1), 8)\n",
        "        # LSTM layer\n",
        "        lstm_out, _ = self.lstm(x, (h_0, c_0))\n",
        "        # select the last layer of the LSTM sequence\n",
        "        lstm_out = lstm_out[-1, :, :]\n",
        "        # linear layer\n",
        "        output = self.linear(lstm_out)\n",
        "\n",
        "\n",
        "        # define sigmoid activation and apply it to the output\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        return sigmoid(output).squeeze()\n",
        "\n",
        "# create the DataLoader using tensor Dataset\n",
        "train_loader = DataLoader(TensorDataset(torch.LongTensor(x_train), torch.FloatTensor(y_train)), batch_size=64, shuffle=True)\n",
        "\n",
        "# Create an instance of LSTM model, an Adam optimizer and BCE loss\n",
        "model = LSTMClassifier(1000, 8, 8, 1)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "best_accuracy = 0\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    # set the model to training mode\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        # calculate the zero gradient\n",
        "        optimizer.zero_grad()\n",
        "        # we then forward pass\n",
        "        outputs = model(inputs)\n",
        "        # calculate our loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backpropogration\n",
        "        loss.backward()\n",
        "        # optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        # add the loss of current item to total\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # convert outputs to match labels as binary predictions\n",
        "        predicted = torch.round(outputs)\n",
        "\n",
        "        # sum of total correct predictions\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        # sum of total\n",
        "        total += labels.size(0)\n",
        "    # calculate the accuracy of the model\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # conditional statements to keep track of best accuracy of the model\n",
        "    if(best_accuracy == 0):\n",
        "      best_accuracy = accuracy\n",
        "    if(accuracy > best_accuracy):\n",
        "      best_accuracy = accuracy\n",
        "\n",
        "print(f\"Best validation accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wirfzkkEsJW4"
      },
      "outputs": [],
      "source": [
        "#import packages here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "N_STATES = 6   # the width of 1-dim world\n",
        "ACTIONS = ['left', 'right']     # the available actions to use\n",
        "EPSILON = 0.9   # the degree of greedy (0＜ε＜1)\n",
        "ALPHA = 0.1     # learning rate (0＜α≤1)\n",
        "GAMMA = 0.9    # discount factor (0＜γ＜1)\n",
        "MAX_EPOCHES = 13   # the max epoches\n",
        "FRESH_TIME = 0.3    # the interval time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbdlOJB7sOn7",
        "outputId": "015efaf2-2624-4363-d00b-1bcd17e8b3f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   left  right\n",
            "0   0.0    0.0\n",
            "1   0.0    0.0\n",
            "2   0.0    0.0\n",
            "3   0.0    0.0\n",
            "4   0.0    0.0\n",
            "5   0.0    0.0\n"
          ]
        }
      ],
      "source": [
        "#define the function here\n",
        "def build_q_table(n_states, actions):\n",
        "   return pd.DataFrame(np.zeros((n_states, len(actions))), columns=actions)\n",
        "\n",
        "q_table = build_q_table(N_STATES, ACTIONS)\n",
        "print(q_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtPWgEtosVho",
        "outputId": "3b9ca1cf-65ca-473a-9f53-5f098b951d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "left\n"
          ]
        }
      ],
      "source": [
        "#define the function here\n",
        "# Given state and Q-table, choose action\n",
        "def choose_action(state, q_table):\n",
        "  # get all q values of the table\n",
        "    state_actions = q_table.iloc[state, :]\n",
        "    if np.random.uniform() < EPSILON or state_actions.all() == 0:\n",
        "      # pick random action\n",
        "        action_name = np.random.choice(ACTIONS)\n",
        "    else:\n",
        "      # choose q with largest value for greedy decision\n",
        "        action_name = state_actions.idxmax()\n",
        "    return action_name\n",
        "\n",
        "sample_action = choose_action(0, q_table)\n",
        "print(sample_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr1W3h6ksY7V",
        "outputId": "7b9d49fb-9742-41f3-9df9-8e1460d23472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 0)\n"
          ]
        }
      ],
      "source": [
        "#define the function here\n",
        "def get_env_feedback(S_current, A):\n",
        "    # This is how agent will interact with the environment\n",
        "    if A == 'right':    # move right\n",
        "        # if we reach the end we will select the next state as terminal\n",
        "        if S_current == N_STATES - 2:\n",
        "            S_next = 'terminal'\n",
        "            # reward is set to 1 for reaching the end\n",
        "            R = 1\n",
        "        # otherwise we move to the next state and set the reward to 0\n",
        "        else:\n",
        "            S_next = S_current + 1\n",
        "            R = 0\n",
        "    else:   # move left\n",
        "        # when S_current is 0 we have reached a wall so we set the next equal to the current to not move\n",
        "        if S_current == 0:\n",
        "            S_next = S_current  # reach the wall\n",
        "        else:\n",
        "          # otherwise if we are not at the wall we will move to the left\n",
        "            S_next = S_current - 1\n",
        "        # reward is always 0 when moving left\n",
        "        R = 0\n",
        "    return S_next, R\n",
        "\n",
        "sample_action = 'left'\n",
        "S_current = 4\n",
        "sample_feedback = get_env_feedback(S_current, sample_action)\n",
        "print(sample_feedback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkBKYpDsbxK"
      },
      "outputs": [],
      "source": [
        "def update_env(S, episode, step_counter):\n",
        "    # This is how environment be updated\n",
        "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
        "    if S == 'terminal':\n",
        "        interaction = '  Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
        "        print('{}\\n'.format(interaction), end='')\n",
        "        time.sleep(2)\n",
        "    else:\n",
        "        env_list[S] = 'o'\n",
        "        interaction = ''.join(env_list)\n",
        "        print('\\r{}'.format(interaction), end='')\n",
        "        time.sleep(FRESH_TIME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_umQ6bsf86"
      },
      "outputs": [],
      "source": [
        "def reinforce_learning():\n",
        "    q_table = build_q_table(N_STATES, ACTIONS)  # build Q-table\n",
        "    for episode in range(MAX_EPOCHES):\n",
        "        step_counter = 0  # counter for counting steps to reach the treasure\n",
        "        S_current = 0  # start from the initial state\n",
        "        is_terminated = False  # flag to continue or stop the loop\n",
        "        update_env(S_current, episode, step_counter)  # update environment\n",
        "        while not is_terminated:\n",
        "            A = choose_action(S_current, q_table)  # choose one action\n",
        "            S_next, R = get_env_feedback(S_current, A)  # take action & get next state and reward\n",
        "            if S_next != 'terminal':  # if the explorer doesn't get to the treasure\n",
        "                q_target = R + GAMMA * q_table.loc[S_next, :].max()  # Bellman equation\n",
        "            else:\n",
        "                q_target = R  # if next state is terminal, the estimated Q-value is the immediate reward\n",
        "                is_terminated = True  # terminate this episode\n",
        "\n",
        "            q_table.loc[S_current, A] += ALPHA * (q_target - q_table.loc[S_current, A])  # update Q-table\n",
        "            S_current = S_next  # move to next state\n",
        "\n",
        "            update_env(S_current, episode, step_counter + 1)\n",
        "            step_counter += 1\n",
        "\n",
        "    return q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U520WbcsjPS",
        "outputId": "fad51795-e1d8-4fd6-f2c8-506b0000ee9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----oT  Episode 1: total_steps = 25\n",
            "----oT  Episode 2: total_steps = 12\n",
            "----oT  Episode 3: total_steps = 54\n",
            "----oT  Episode 4: total_steps = 17\n",
            "----oT  Episode 5: total_steps = 9\n",
            "----oT  Episode 6: total_steps = 25\n",
            "----oT  Episode 7: total_steps = 9\n",
            "----oT  Episode 8: total_steps = 12\n",
            "----oT  Episode 9: total_steps = 9\n",
            "----oT  Episode 10: total_steps = 20\n",
            "----oT  Episode 11: total_steps = 11\n",
            "----oT  Episode 12: total_steps = 11\n",
            "----oT  Episode 13: total_steps = 29\n",
            "\n",
            "Q-table:\n",
            "\n",
            "       left     right\n",
            "0  0.011891  0.033624\n",
            "1  0.014986  0.074258\n",
            "2  0.013679  0.213437\n",
            "3  0.049346  0.421909\n",
            "4  0.088946  0.745813\n",
            "5  0.000000  0.000000\n"
          ]
        }
      ],
      "source": [
        "#main function to run\n",
        "if __name__ == \"__main__\":\n",
        "    q_table = reinforce_learning()\n",
        "    print('\\r\\nQ-table:\\n')\n",
        "    print(q_table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
